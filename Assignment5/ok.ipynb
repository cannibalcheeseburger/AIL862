{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ml-collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"TransUNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.vit_seg_modeling import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_path': 'cityscapes',\n",
    "    'device': 'cuda',\n",
    "    'clip_threshold': 0.82,\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 10,\n",
    "    'num_classes': 35,\n",
    "    'image_size': (512, 512),\n",
    "    'transunet_config': {\n",
    "        'vit_name': 'R50-ViT-B_16',\n",
    "        'vit_patches_size': 16,\n",
    "        'hidden_size': 768  # ViT-Base dimension\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesData:\n",
    "    def __init__(self):\n",
    "        self.item_a = {\n",
    "            'image': f\"{CONFIG['dataset_path']}/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\",\n",
    "            'mask': f\"{CONFIG['dataset_path']}/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_labelIds.png\"\n",
    "        }\n",
    "        \n",
    "        self.item_c = self._load_unlabeled()\n",
    "        self.item_d = self._load_test_set()\n",
    "    \n",
    "    def _load_unlabeled(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/test/\"\n",
    "        for city in os.listdir(base_path)[:2]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "    \n",
    "    def _load_test_set(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/val\"\n",
    "        for city in os.listdir(base_path)[:1]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CityscapesData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudolabelGenerator:\n",
    "    def __init__(self):\n",
    "        self.sam = sam_model_registry['vit_h'](\n",
    "            checkpoint='sam_vit_h_4b8939.pth'\n",
    "        ).to(CONFIG['device'])\n",
    "        self.clip_model, self.preprocess = clip.load(\n",
    "            'ViT-B/32', device=CONFIG['device'])\n",
    "        self.mask_generator = SamAutomaticMaskGenerator(self.sam)\n",
    "        \n",
    "    def _clip_similarity(self, image_patch):\n",
    "        # Convert numpy array to PIL Image\n",
    "        if isinstance(image_patch, np.ndarray):\n",
    "            # Handle different color formats\n",
    "            if image_patch.shape[-1] == 3:  # BGR to RGB\n",
    "                image_patch = cv2.cvtColor(image_patch, cv2.COLOR_BGR2RGB)\n",
    "            image_patch = Image.fromarray(image_patch.astype('uint8'))\n",
    "            \n",
    "        image_tensor = self.preprocess(image_patch).unsqueeze(0).to(CONFIG['device'])\n",
    "        text = clip.tokenize([\"Urban street scene with vehicles, pedestrians, and road infrastructure\"]).to(CONFIG['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image_tensor)\n",
    "            text_features = self.clip_model.encode_text(text)\n",
    "            \n",
    "        return torch.cosine_similarity(image_features, text_features).item()\n",
    "    \n",
    "    def generate(self, image_paths):\n",
    "        pseudo_labels = []\n",
    "        \n",
    "        for path in tqdm(image_paths, desc=\"Generating pseudo-labels\"):\n",
    "            image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            masks = self.mask_generator.generate(image)\n",
    "            \n",
    "            valid_masks = []\n",
    "            for mask in masks:\n",
    "                x1, y1, w, h = mask['bbox']\n",
    "                if w == 0 or h == 0:\n",
    "                    continue  # Skip invalid masks\n",
    "                    \n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                patch = image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Handle empty patches\n",
    "                if patch.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                similarity = self._clip_similarity(patch)\n",
    "                if similarity > CONFIG['clip_threshold']:\n",
    "                    valid_masks.append(mask['segmentation'])\n",
    "            \n",
    "            if valid_masks:\n",
    "                combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                for m in valid_masks:\n",
    "                    combined_mask[m] = 1\n",
    "                pseudo_labels.append((path, combined_mask))\n",
    "        \n",
    "        return pseudo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 20/20 [05:19<00:00, 15.96s/it]\n"
     ]
    }
   ],
   "source": [
    "pl_generator = PseudolabelGenerator()\n",
    "c_prime = pl_generator.generate(data.item_c[:20])  # Use subset for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, mask = self.data[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image = torch.tensor(image).permute(2,0,1).float() / 255.0\n",
    "        \n",
    "        mask = cv2.resize(mask, CONFIG['image_size'])\n",
    "        return image, torch.tensor(mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    # Model setup\n",
    "\n",
    "    # For model loading:\n",
    "    model = VisionTransformer(\n",
    "        config=CONFIG['transunet_config'],  # ✅ Config object\n",
    "        img_size=CONFIG['image_size'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        in_channels=3  # RGB input\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    \n",
    "    # Training setup\n",
    "    dataset = SegmentationDataset(train_data)\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        for images, masks in loader:\n",
    "            images = images.to(CONFIG['device'])\n",
    "            masks = masks.to(CONFIG['device'])\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(loader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VisionTransformer.__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_prime\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_data):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Model setup\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# For model loading:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransunet_config\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Config object\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# RGB input\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Training setup\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m SegmentationDataset(train_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: VisionTransformer.__init__() got an unexpected keyword argument 'in_channels'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = train_model(c_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_paths):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "    \n",
    "    for path in tqdm(test_paths, desc=\"Evaluating\"):\n",
    "        # Load and prepare image\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image_tensor = torch.tensor(image).permute(2,0,1).float().unsqueeze(0) / 255.0\n",
    "        image_tensor = image_tensor.to(CONFIG['device'])\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor).logits\n",
    "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_path = path.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png')\n",
    "        gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])\n",
    "        \n",
    "        # Calculate IoU\n",
    "        intersection = np.logical_and(pred_mask, gt_mask)\n",
    "        union = np.logical_or(pred_mask, gt_mask)\n",
    "        ious.append(np.sum(intersection) / np.sum(union))\n",
    "    \n",
    "    return {\n",
    "        'mean_iou': np.mean(ious),\n",
    "        'std_iou': np.std(ious),\n",
    "        'max_iou': np.max(ious),\n",
    "        'min_iou': np.min(ious)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, data.item_d[:10])  # Use subset for demo\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:8}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image_path, pred_mask, gt_mask):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, CONFIG['image_size'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Input Image')\n",
    "    \n",
    "    ax[1].imshow(pred_mask)\n",
    "    ax[1].set_title('Predicted Mask')\n",
    "    \n",
    "    ax[2].imshow(gt_mask)\n",
    "    ax[2].set_title('Ground Truth')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = data.item_d[0]\n",
    "pred_mask = model(torch.tensor(cv2.imread(test_image)).permute(2,0,1).float().unsqueeze(0).to(CONFIG['device'])/255.0).logits.argmax(1).squeeze().cpu().numpy()\n",
    "gt_mask = cv2.imread(test_image.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png'), cv2.IMREAD_GRAYSCALE)\n",
    "gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample(test_image, pred_mask, gt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
