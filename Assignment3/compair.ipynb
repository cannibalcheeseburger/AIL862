{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(root=\"data/synthetic_imagenette/\", transform=transform)\n",
    "test_dataset = ImageFolder(root=\"data/real/imagenette/val\", transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [10,20,35,50,75,100, ]#300, 500, 800, 1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, batch_size, val_split=0.2):\n",
    "    # Calculate the number of samples to use for validation\n",
    "    n_total = len(train_dataset)\n",
    "    n_val = int(val_split * n_total)\n",
    "    n_train = n_total - n_val\n",
    "\n",
    "    # Split the train dataset into train and validation\n",
    "    train_data, val_data = random_split(train_dataset, [n_train, n_val])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader,test_loader = create_data_loaders(train_dataset,test_dataset,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(sample_sizes):\n",
    "    results = {}\n",
    "    for sample_size in sample_sizes:\n",
    "        # Subset the datasets\n",
    "        train_subset = Subset(train_dataset, list(range(sample_size)))\n",
    "        \n",
    "        # Create data loaders with validation split\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(train_subset, test_dataset, batch_size)\n",
    "\n",
    "        # Model setup\n",
    "        model = resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 3)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        # Training and evaluation\n",
    "        num_epochs = 10\n",
    "        train_losses, val_losses, test_losses = [], [], []\n",
    "        train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "        # Test\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        print(f\"Final Test Results:\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        results[sample_size] = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"val_accuracies\": val_accuracies,\n",
    "            \"test_accuracies\": test_accuracies\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 1.2223, Train Acc: 0.00%\n",
      "  Val Loss: 1.2946, Val Acc: 0.00%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 1.0568, Train Acc: 62.50%\n",
      "  Val Loss: 1.1838, Val Acc: 0.00%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.9217, Train Acc: 75.00%\n",
      "  Val Loss: 1.0475, Val Acc: 50.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.7638, Train Acc: 100.00%\n",
      "  Val Loss: 0.8243, Val Acc: 100.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.6561, Train Acc: 100.00%\n",
      "  Val Loss: 0.7937, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.5593, Train Acc: 100.00%\n",
      "  Val Loss: 0.4986, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.4668, Train Acc: 100.00%\n",
      "  Val Loss: 0.4565, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.3722, Train Acc: 100.00%\n",
      "  Val Loss: 0.3465, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.3083, Train Acc: 100.00%\n",
      "  Val Loss: 0.3472, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.2609, Train Acc: 100.00%\n",
      "  Val Loss: 0.2789, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 1.5427, Test Acc: 30.64%\n",
      "Epoch 1/10:\n",
      "  Train Loss: 0.8804, Train Acc: 93.75%\n",
      "  Val Loss: 1.1490, Val Acc: 25.00%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 0.7481, Train Acc: 100.00%\n",
      "  Val Loss: 0.9522, Val Acc: 75.00%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.6232, Train Acc: 100.00%\n",
      "  Val Loss: 0.7543, Val Acc: 100.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.5205, Train Acc: 100.00%\n",
      "  Val Loss: 0.7228, Val Acc: 100.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.4286, Train Acc: 100.00%\n",
      "  Val Loss: 0.6377, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.3599, Train Acc: 100.00%\n",
      "  Val Loss: 0.4559, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.2867, Train Acc: 100.00%\n",
      "  Val Loss: 0.4376, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.2396, Train Acc: 100.00%\n",
      "  Val Loss: 0.4271, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.1948, Train Acc: 100.00%\n",
      "  Val Loss: 0.3948, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.1603, Train Acc: 100.00%\n",
      "  Val Loss: 0.2514, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 1.7233, Test Acc: 30.64%\n",
      "Epoch 1/10:\n",
      "  Train Loss: 0.8362, Train Acc: 82.14%\n",
      "  Val Loss: 0.8959, Val Acc: 85.71%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 0.6992, Train Acc: 100.00%\n",
      "  Val Loss: 0.8102, Val Acc: 100.00%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.5992, Train Acc: 100.00%\n",
      "  Val Loss: 0.6400, Val Acc: 100.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.4866, Train Acc: 100.00%\n",
      "  Val Loss: 0.5822, Val Acc: 100.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.3967, Train Acc: 100.00%\n",
      "  Val Loss: 0.4749, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.3315, Train Acc: 100.00%\n",
      "  Val Loss: 0.4417, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.2692, Train Acc: 100.00%\n",
      "  Val Loss: 0.4289, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.2151, Train Acc: 100.00%\n",
      "  Val Loss: 0.3136, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.1784, Train Acc: 100.00%\n",
      "  Val Loss: 0.2753, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.1406, Train Acc: 100.00%\n",
      "  Val Loss: 0.2061, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 1.7218, Test Acc: 30.64%\n",
      "Epoch 1/10:\n",
      "  Train Loss: 1.2031, Train Acc: 15.00%\n",
      "  Val Loss: 1.1603, Val Acc: 20.00%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 1.0307, Train Acc: 45.00%\n",
      "  Val Loss: 1.0619, Val Acc: 60.00%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.8745, Train Acc: 90.00%\n",
      "  Val Loss: 0.9267, Val Acc: 100.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.7318, Train Acc: 100.00%\n",
      "  Val Loss: 0.7529, Val Acc: 100.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.6113, Train Acc: 100.00%\n",
      "  Val Loss: 0.6463, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.4897, Train Acc: 100.00%\n",
      "  Val Loss: 0.5655, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.4015, Train Acc: 100.00%\n",
      "  Val Loss: 0.4342, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.3255, Train Acc: 100.00%\n",
      "  Val Loss: 0.3521, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.2577, Train Acc: 100.00%\n",
      "  Val Loss: 0.2581, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.2027, Train Acc: 100.00%\n",
      "  Val Loss: 0.2028, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 1.5892, Test Acc: 30.64%\n",
      "Epoch 1/10:\n",
      "  Train Loss: 1.2159, Train Acc: 1.67%\n",
      "  Val Loss: 1.1947, Val Acc: 0.00%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 1.0430, Train Acc: 33.33%\n",
      "  Val Loss: 1.0679, Val Acc: 33.33%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.8830, Train Acc: 76.67%\n",
      "  Val Loss: 0.9360, Val Acc: 60.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.7405, Train Acc: 91.67%\n",
      "  Val Loss: 0.7341, Val Acc: 100.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.6127, Train Acc: 100.00%\n",
      "  Val Loss: 0.6360, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.4954, Train Acc: 100.00%\n",
      "  Val Loss: 0.5383, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.3980, Train Acc: 100.00%\n",
      "  Val Loss: 0.4380, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.3161, Train Acc: 100.00%\n",
      "  Val Loss: 0.3447, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.2460, Train Acc: 100.00%\n",
      "  Val Loss: 0.2341, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.1958, Train Acc: 100.00%\n",
      "  Val Loss: 0.2078, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 1.8116, Test Acc: 30.64%\n",
      "Epoch 1/10:\n",
      "  Train Loss: 1.4428, Train Acc: 1.25%\n",
      "  Val Loss: 1.4212, Val Acc: 0.00%\n",
      "Epoch 2/10:\n",
      "  Train Loss: 1.0857, Train Acc: 12.50%\n",
      "  Val Loss: 1.0973, Val Acc: 45.00%\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.7866, Train Acc: 82.50%\n",
      "  Val Loss: 0.9350, Val Acc: 70.00%\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.5455, Train Acc: 98.75%\n",
      "  Val Loss: 0.5980, Val Acc: 95.00%\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.3651, Train Acc: 100.00%\n",
      "  Val Loss: 0.3733, Val Acc: 100.00%\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.2357, Train Acc: 100.00%\n",
      "  Val Loss: 0.2054, Val Acc: 100.00%\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.1527, Train Acc: 100.00%\n",
      "  Val Loss: 0.1091, Val Acc: 100.00%\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.1003, Train Acc: 100.00%\n",
      "  Val Loss: 0.0695, Val Acc: 100.00%\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.0678, Train Acc: 100.00%\n",
      "  Val Loss: 0.0444, Val Acc: 100.00%\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.0459, Train Acc: 100.00%\n",
      "  Val Loss: 0.0350, Val Acc: 100.00%\n",
      "Final Test Results:\n",
      "  Test Loss: 2.6195, Test Acc: 30.64%\n"
     ]
    }
   ],
   "source": [
    "results = train_and_evaluate(sample_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size 10:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n",
      "Sample size 20:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n",
      "Sample size 35:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n",
      "Sample size 50:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n",
      "Sample size 75:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n",
      "Sample size 100:\n",
      "  Final train accuracy: 100.00%\n",
      "  Final val accuracy: 100.00%\n",
      "  Final test accuracy: 30.64%\n"
     ]
    }
   ],
   "source": [
    "for size, data in results.items():\n",
    "    print(f\"Sample size {size}:\")\n",
    "    print(f\"  Final train accuracy: {data['train_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"  Final val accuracy: {data['val_accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    print(f\"  Final test accuracy: {data['test_accuracies'][-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, data \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_losses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msize\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\matplotlib\\pyplot.py:3829\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3821\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3827\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3828\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kashp\\miniconda3\\envs\\cva3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (10,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "for size, data in results.items():\n",
    "    plt.plot(range(1, 6), data[\"train_losses\"], label=f\"Train {size}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for size, data in results.items():\n",
    "    plt.plot(range(1, 6), data[\"test_losses\"], label=f\"Test {size}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "for size, data in results.items():\n",
    "    plt.plot(range(1, 6), data[\"train_accuracies\"], label=f\"Train {size}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for size, data in results.items():\n",
    "    plt.plot(range(1, 6), data[\"test_accuracies\"], label=f\"Test {size}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cva3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
