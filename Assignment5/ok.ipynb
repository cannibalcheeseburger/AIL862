{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_path': os.environ.get('CITYSCAPES_DATASET', './cityscapes'),\n",
    "    'device': 'mps' if torch.backends.mps.is_available() else 'cpu',\n",
    "    'clip_threshold': 0.82,\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 10,\n",
    "    'num_classes': 35,\n",
    "    'image_size': (512, 512)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesData:\n",
    "    def __init__(self):\n",
    "        self.item_a = {\n",
    "            'image': f\"{CONFIG['dataset_path']}/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\",\n",
    "            'mask': f\"{CONFIG['dataset_path']}/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_labelIds.png\"\n",
    "        }\n",
    "        \n",
    "        self.item_c = self._load_unlabeled()\n",
    "        self.item_d = self._load_test_set()\n",
    "    \n",
    "    def _load_unlabeled(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/train_extra\"\n",
    "        for city in os.listdir(base_path)[:2]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "    \n",
    "    def _load_test_set(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/val\"\n",
    "        for city in os.listdir(base_path)[:1]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cityscapes/leftImg8bit/train_extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43mCityscapesData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mCityscapesData.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m.item_a = {\n\u001b[32m      4\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mdataset_path\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mdataset_path\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_labelIds.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     }\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mself\u001b[39m.item_c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_unlabeled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.item_d = \u001b[38;5;28mself\u001b[39m._load_test_set()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mCityscapesData._load_unlabeled\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     12\u001b[39m paths = []\n\u001b[32m     13\u001b[39m base_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mdataset_path\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/leftImg8bit/train_extra\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m[:\u001b[32m2\u001b[39m]:  \u001b[38;5;66;03m# Limit for demo\u001b[39;00m\n\u001b[32m     15\u001b[39m     city_path = os.path.join(base_path, city)\n\u001b[32m     16\u001b[39m     paths.extend([os.path.join(city_path, f) \n\u001b[32m     17\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(city_path)[:\u001b[32m50\u001b[39m]])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './cityscapes/leftImg8bit/train_extra'"
     ]
    }
   ],
   "source": [
    "data = CityscapesData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudolabelGenerator:\n",
    "    def __init__(self):\n",
    "        self.sam = sam_model_registry['vit_h'](\n",
    "            checkpoint='sam_vit_h_4b8939.pth'\n",
    "        ).to(CONFIG['device'])\n",
    "        self.clip_model, self.preprocess = clip.load(\n",
    "            'ViT-B/32', device=CONFIG['device'])\n",
    "        self.mask_generator = SamAutomaticMaskGenerator(self.sam)\n",
    "        \n",
    "    def _clip_similarity(self, image_patch):\n",
    "        image_tensor = self.preprocess(image_patch).unsqueeze(0).to(CONFIG['device'])\n",
    "        text = clip.tokenize([\"Urban street scene with vehicles, pedestrians, and road infrastructure\"]).to(CONFIG['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image_tensor)\n",
    "            text_features = self.clip_model.encode_text(text)\n",
    "            \n",
    "        return torch.cosine_similarity(image_features, text_features).item()\n",
    "    \n",
    "    def generate(self, image_paths):\n",
    "        pseudo_labels = []\n",
    "        \n",
    "        for path in tqdm(image_paths, desc=\"Generating pseudo-labels\"):\n",
    "            image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            masks = self.mask_generator.generate(image)\n",
    "            \n",
    "            valid_masks = []\n",
    "            for mask in masks:\n",
    "                x1, y1, w, h = mask['bbox']\n",
    "                patch = image[y1:y1+h, x1:x1+w]\n",
    "                if patch.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                similarity = self._clip_similarity(patch)\n",
    "                if similarity > CONFIG['clip_threshold']:\n",
    "                    valid_masks.append(mask['segmentation'])\n",
    "            \n",
    "            if valid_masks:\n",
    "                combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                for m in valid_masks:\n",
    "                    combined_mask[m] = 1\n",
    "                pseudo_labels.append((path, combined_mask))\n",
    "        \n",
    "        return pseudo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_generator = PseudolabelGenerator()\n",
    "c_prime = pl_generator.generate(data.item_c[:20])  # Use subset for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, mask = self.data[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image = torch.tensor(image).permute(2,0,1).float() / 255.0\n",
    "        \n",
    "        mask = cv2.resize(mask, CONFIG['image_size'])\n",
    "        return image, torch.tensor(mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    # Model setup\n",
    "    config = AutoConfig.from_pretrained(\"microsoft/transunet-medical\")\n",
    "\n",
    "    # For model loading:\n",
    "    model = AutoModel.from_pretrained(\n",
    "        \"microsoft/transunet-medical\",\n",
    "        config=config\n",
    "    )\n",
    "    model.classifier = torch.nn.Conv2d(768, CONFIG['num_classes'], 1)\n",
    "    model = model.to(CONFIG['device'])\n",
    "    \n",
    "    # Training setup\n",
    "    dataset = SegmentationDataset(train_data)\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for images, masks in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(CONFIG['device'])\n",
    "            masks = masks.to(CONFIG['device'])\n",
    "            \n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(loader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(c_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_paths):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "    \n",
    "    for path in tqdm(test_paths, desc=\"Evaluating\"):\n",
    "        # Load and prepare image\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image_tensor = torch.tensor(image).permute(2,0,1).float().unsqueeze(0) / 255.0\n",
    "        image_tensor = image_tensor.to(CONFIG['device'])\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor).logits\n",
    "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_path = path.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png')\n",
    "        gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])\n",
    "        \n",
    "        # Calculate IoU\n",
    "        intersection = np.logical_and(pred_mask, gt_mask)\n",
    "        union = np.logical_or(pred_mask, gt_mask)\n",
    "        ious.append(np.sum(intersection) / np.sum(union))\n",
    "    \n",
    "    return {\n",
    "        'mean_iou': np.mean(ious),\n",
    "        'std_iou': np.std(ious),\n",
    "        'max_iou': np.max(ious),\n",
    "        'min_iou': np.min(ious)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, data.item_d[:10])  # Use subset for demo\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:8}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image_path, pred_mask, gt_mask):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, CONFIG['image_size'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Input Image')\n",
    "    \n",
    "    ax[1].imshow(pred_mask)\n",
    "    ax[1].set_title('Predicted Mask')\n",
    "    \n",
    "    ax[2].imshow(gt_mask)\n",
    "    ax[2].set_title('Ground Truth')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = data.item_d[0]\n",
    "pred_mask = model(torch.tensor(cv2.imread(test_image)).permute(2,0,1).float().unsqueeze(0).to(CONFIG['device'])/255.0).logits.argmax(1).squeeze().cpu().numpy()\n",
    "gt_mask = cv2.imread(test_image.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png'), cv2.IMREAD_GRAYSCALE)\n",
    "gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample(test_image, pred_mask, gt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cva5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
