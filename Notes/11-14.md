Here's a detailed explanation of the key concepts from your lecture slides, organized by topic:

---

## **1. GAN Issues & Solutions (Lecture 11)**

### **Key Challenges**
1. **Mode Collapse**  
   - Generator produces limited sample diversity (e.g., only generates 2-3 MNIST digits).  
   - *Code Example:* [GAN Mode Collapse Notebook](https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L17_gans/code/gan-halfcnn-mnist-mode-collapse.ipynb) demonstrates this phenomenon.  

2. **Oscillating Losses**  
   - Discriminator (D) and generator (G) losses fluctuate instead of converging.  
   - Caused by imbalanced D/G training dynamics.  

3. **Overpowered Discriminator**  
   - D becomes too accurate, leaving G unable to learn.  

### **Solutions**  
| Technique                | Implementation                                                                 |
|--------------------------|--------------------------------------------------------------------------------|
| **Reduced D Updates**    | Train G 5x more frequently than D to let G catch up.                          |
| **Sample Buffer**        | Store historical generated images; mix with new samples during D training.    |
| **Progressive Training** | Start with 32x32 images → gradually scale to 1024x1024 (e.g., StyleGAN).      |
| **Weight Clipping**      | Limit discriminator weights to [-0.01, 0.01] for stability.                   |

---

## **2. Self-Supervised Learning (Lectures 12-13)**

### **Core Concepts**
- **Pretext Tasks:** Unsupervised tasks that teach models useful visual features.  
- **Contrastive Learning:** Learn by comparing similar/dissimilar pairs.  

### **Key Methods**
| Method                   | Implementation                                                                 |
|--------------------------|--------------------------------------------------------------------------------|
| **Rotation Prediction**  | Rotate images (0°, 90°, 180°, 270°); train model to predict rotation angle.   |
| **Jigsaw Puzzles**       | Shuffle image patches; train model to reconstruct original order.             |
| **Geolocation Classification** | Cluster images by GPS coordinates; predict location clusters.             |
| **SimCLR**               | Maximize similarity between augmented views of the same image:                |
|                          | `loss = -log(exp(sim(z_i,z_j)/τ) / ∑ exp(sim(z_i,z_k)/τ))`                    |
| **MoCo**                 | Uses momentum encoder + queue of negative samples for efficient contrastive learning. |

### **MNIST Rotation Example**
- **Baseline:** 95% accuracy with random initialization + linear probing.  
- **Rotation Pretext:** 97.5% accuracy after pretraining on rotation prediction.  
```python
# Rotation pretext training
pretextTarget = torch.randint(0, 4, (batch_size,))  # 0-3 rotation classes
rotated_images = rotate(images, pretextTarget)  
model = train_classifier(rotated_images, pretextTarget)
```

---

## **3. Vision Transformers (ViT) (Lecture 14)**

### **Architecture**
1. **Patch Embedding**  
   - Split image into 16x16 patches → flatten → linear projection to embeddings.  
   ```python
   x = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=16, p2=16)(img)
   x = nn.Linear(patch_dim, dim)(x)
   ```
2. **Positional Embedding**  
   - Add learnable positional vectors to preserve spatial information.  
3. **Transformer Encoder**  
   - Multi-head self-attention + MLP blocks.  
   ```python
   class Transformer(nn.Module):
       def __init__(self, dim, depth, heads, ...):
           self.layers = nn.ModuleList([Attention(dim, heads), FeedForward(dim)])
   ```

### **Variants**
| Model       | Layers | Heads | Parameters |
|-------------|--------|-------|------------|
| ViT-Base    | 12     | 12    | 86M        |
| ViT-Large   | 24     | 16    | 307M       |
| ViT-Huge    | 32     | 16    | 632M       |

### **Key Insights**
- Requires massive pretraining data (JFT-300M > ImageNet-21K > ImageNet).  
- Outperforms CNNs when pretrained on >100M images.  

---

## **4. Contrastive Learning Details**

### **SimCLR Workflow**
1. **Augmentation**  
   - Apply random crop + color jitter + Gaussian blur.  
2. **Projection Head**  
   - Maps encoder outputs to contrastive space:  
     ```python
     z = nn.Linear(2048, 128)(features)  # Example projection
     ```
3. **NT-Xent Loss**  
   - Temperature-scaled cross-entropy for positive pairs.  

### **MoCo Innovations**
- **Momentum Encoder:** EMA-updated encoder for stable negative samples.  
- **Queue:** Stores 65,536 negative samples from previous batches.  

---

## **Key Takeaways**
1. **GAN Stability:** Requires careful balancing of D/G training + progressive scaling.  
2. **SSL Power:** Pretext tasks (rotation/jigsaw) provide 2-5% accuracy boosts in downstream tasks.  
3. **ViT Scalability:** Performance improves with model size/data, but needs massive pretraining.  
4. **Contrastive Learning:** SimCLR/MoCo enable label-efficient representation learning.  

Let me know if you'd like further clarification on specific diagrams or code implementations!

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/50850777/38106f00-9850-4c5a-8f74-9c818b71f11f/lecture11.pdf
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/50850777/f14de25b-6384-4d80-b7b0-e0819b9aa5f0/lecture12.pdf
[3] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/50850777/35a738f2-a58a-4d48-9f27-ad32c8f3932a/lecture13.pdf
[4] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/50850777/4f11748c-2b3d-40a5-afbd-07913638b02f/lecture14.pdf

---
Answer from Perplexity: pplx.ai/share