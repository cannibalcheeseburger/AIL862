{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11544ca10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.image_paths = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(('.jpg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # No label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # âœ… Ensure tensor conversion before normalization\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "target_transform = transforms.Lambda(lambda y: torch.tensor(y, dtype=torch.long))\n",
    "\n",
    "# Load datasets\n",
    "synthetic_dataset = ImageFolder(root=\"data/synthetic/cifar10\", \n",
    "                                transform=transform_test,\n",
    "                                target_transform=target_transform)\n",
    "unlabeled_dataset = UnlabeledDataset(\"data/real/unlabelled\", transform=transform_test)\n",
    "test_dataset = ImageFolder(root=\"data/real/animal_data\", transform=transform_test)\n",
    "\n",
    "batch_size = 32\n",
    "synthetic_loader = DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size=1280):\n",
    "        super(DomainDiscriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        mobilenet = mobilenet_v2(pretrained=True)\n",
    "        self.features = mobilenet.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = self.avgpool(features)\n",
    "        features = torch.flatten(features, 1)\n",
    "        class_output = self.classifier(features)\n",
    "        return features, class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(model, discriminator, combined_loader, unlabeled_loader, num_epochs):\n",
    "    optimizer_G = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "        total_loss_G, total_loss_D, total_loss_cls = 0, 0, 0\n",
    "        \n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        \n",
    "        for combined_data, combined_labels in combined_loader:\n",
    "            try:\n",
    "                target_data = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                target_data = next(unlabeled_iter)\n",
    "            \n",
    "            batch_size = min(combined_data.size(0), target_data.size(0))\n",
    "            combined_data = combined_data[:batch_size].to(device)\n",
    "            combined_labels = combined_labels[:batch_size].to(device)\n",
    "            target_data = target_data[:batch_size].to(device)\n",
    "            \n",
    "            # Train discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            combined_features, combined_outputs = model(combined_data)\n",
    "            target_features, _ = model(target_data)\n",
    "            \n",
    "            domain_pred_combined = discriminator(combined_features.detach())\n",
    "            domain_pred_target = discriminator(target_features.detach())\n",
    "            domain_pred_combined = domain_pred_combined.view(-1)\n",
    "            domain_pred_target = domain_pred_target.view(-1)\n",
    "\n",
    "            loss_D = -torch.mean(torch.log(torch.sigmoid(domain_pred_combined) + 1e-10) + \n",
    "                                 torch.log(1 - torch.sigmoid(domain_pred_target) + 1e-10))\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train generator (feature extractor)\n",
    "            optimizer_G.zero_grad()\n",
    "            combined_features, combined_outputs = model(combined_data)\n",
    "            target_features, _ = model(target_data)\n",
    "            \n",
    "            loss_cls = criterion(combined_outputs, combined_labels)\n",
    "            domain_pred_target = discriminator(target_features)\n",
    "            loss_adv = -torch.mean(torch.log(torch.sigmoid(domain_pred_target.view(-1)) + 1e-10))\n",
    "            \n",
    "            loss_G = loss_cls + 0.1 * loss_adv\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            total_loss_G += loss_G.item()\n",
    "            total_loss_D += loss_D.item()\n",
    "            total_loss_cls += loss_cls.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss G: {total_loss_G/len(combined_loader):.4f}, \"\n",
    "              f\"Loss D: {total_loss_D/len(combined_loader):.4f}, \"\n",
    "              f\"Loss Cls: {total_loss_cls/len(combined_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kash/miniconda/envs/cva3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kash/miniconda/envs/cva3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model.pth\n"
     ]
    }
   ],
   "source": [
    "def load_model(model, path, device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# Load the saved model\n",
    "num_classes = 3\n",
    "model = FeatureExtractor(num_classes=num_classes).to(device)\n",
    "model = load_model(model, 'model.pth', device)\n",
    "\n",
    "discriminator = DomainDiscriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generate_pseudo_labels(model, dataloader, threshold=0.7):\n",
    "    model.eval()\n",
    "    pseudo_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            features, outputs = model(images)  # Unpack the tuple\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, pseudo_labels = torch.max(probabilities, dim=1)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                if confidence[i] > threshold:\n",
    "                    pseudo_data.append((images[i].cpu(), pseudo_labels[i].cpu()))\n",
    "    \n",
    "    return pseudo_data\n",
    "\n",
    "\n",
    "class PseudoLabeledDataset(Dataset):\n",
    "    def __init__(self, pseudo_data, transform=None):\n",
    "        self.pseudo_data = pseudo_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pseudo_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.pseudo_data[idx]\n",
    "        \n",
    "        # Ensure image is a tensor\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Ensure label is a tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33514/4191723487.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "pseudo_data = generate_pseudo_labels(model, unlabeled_loader, threshold=0.51)\n",
    "pseudo_dataset = PseudoLabeledDataset(pseudo_data)\n",
    "combined_dataset = ConcatDataset([synthetic_dataset, pseudo_dataset])\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "for (combined_data, combined_labels), target_data in zip(combined_loader, cycle(unlabeled_loader)):\n",
    "    batch_size = min(combined_data.size(0), target_data.size(0))\n",
    "    combined_data = combined_data[:batch_size]\n",
    "    combined_labels = combined_labels[:batch_size]\n",
    "    target_data = target_data[:batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33514/4191723487.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss G: 0.3729, Loss D: 1.3626, Loss Cls: 0.2980\n",
      "Epoch [2/10], Loss G: 0.1869, Loss D: 1.3682, Loss Cls: 0.1141\n",
      "Epoch [3/10], Loss G: 0.1202, Loss D: 1.3680, Loss Cls: 0.0478\n",
      "Epoch [4/10], Loss G: 0.1305, Loss D: 1.3802, Loss Cls: 0.0593\n",
      "Epoch [5/10], Loss G: 0.1794, Loss D: 1.3683, Loss Cls: 0.1069\n",
      "Epoch [6/10], Loss G: 0.1395, Loss D: 1.3671, Loss Cls: 0.0662\n",
      "Epoch [7/10], Loss G: 0.1805, Loss D: 1.3693, Loss Cls: 0.1059\n",
      "Epoch [8/10], Loss G: 0.1720, Loss D: 1.3599, Loss Cls: 0.0976\n",
      "Epoch [9/10], Loss G: 0.1593, Loss D: 1.3637, Loss Cls: 0.0862\n",
      "Epoch [10/10], Loss G: 0.1826, Loss D: 1.3663, Loss Cls: 0.1090\n"
     ]
    }
   ],
   "source": [
    "train_adversarial(model, discriminator, combined_loader, unlabeled_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current seed: 42\n"
     ]
    }
   ],
   "source": [
    "current_seed = torch.initial_seed()\n",
    "print(f\"Current seed: {current_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 82.46% | Test Loss: 0.5370\n",
      "\n",
      "Improved model saved as 'domain_adapted_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        _, outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the improved model\n",
    "torch.save(model.state_dict(), \"domain_adapted_model.pth\")\n",
    "print(\"\\nImproved model saved as 'domain_adapted_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cva3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
