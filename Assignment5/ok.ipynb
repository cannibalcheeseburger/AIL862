{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/Beckschen/TransUNet.git\n",
    "#!pip install tensorboard tensorboardX ml-collections medpy SimpleITK scipy h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"TransUNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.vit_seg_modeling import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://download.pytorch.org/models/resnet50-0676ba61.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_path': './cityscapes',  # Replace with actual Cityscapes dataset path\n",
    "    'transunet_config': {\n",
    "        'img_size': 512,\n",
    "        'in_channels': 3,\n",
    "        'num_classes': 35,\n",
    "        'hidden_size': 768,\n",
    "        'num_attention_heads': 12,\n",
    "        'classifier': 'seg',\n",
    "        'n_skip': 3,\n",
    "        'vit_name': 'R50-ViT-B_16',\n",
    "        'patches': {\"grid\": (16, 16)},\n",
    "        'block_units': [3, 4, 6, 3],\n",
    "        'width_factor': 4,\n",
    "        'pretrained_path': './pretrained/resnet50.pth'\n",
    "        },\n",
    "    'device': 'cuda',\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 10,\n",
    "    'clip_threshold': 0.82,\n",
    "    'image_size': (512, 512)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesData:\n",
    "    def __init__(self):\n",
    "        self.item_a = {\n",
    "            'image': f\"{CONFIG['dataset_path']}/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\",\n",
    "            'mask': f\"{CONFIG['dataset_path']}/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_labelIds.png\"\n",
    "        }\n",
    "        \n",
    "        self.item_c = self._load_unlabeled()\n",
    "        self.item_d = self._load_test_set()\n",
    "    \n",
    "    def _load_unlabeled(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/test/\"\n",
    "        for city in os.listdir(base_path)[:2]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "    \n",
    "    def _load_test_set(self):\n",
    "        paths = []\n",
    "        base_path = f\"{CONFIG['dataset_path']}/leftImg8bit/val\"\n",
    "        for city in os.listdir(base_path)[:1]:  # Limit for demo\n",
    "            city_path = os.path.join(base_path, city)\n",
    "            paths.extend([os.path.join(city_path, f) \n",
    "                         for f in os.listdir(city_path)[:50]])\n",
    "        return paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CityscapesData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudolabelGenerator:\n",
    "    def __init__(self):\n",
    "        self.sam = sam_model_registry['vit_h'](\n",
    "            checkpoint='sam_vit_h_4b8939.pth'\n",
    "        ).to(CONFIG['device'])\n",
    "        self.clip_model, self.preprocess = clip.load(\n",
    "            'ViT-B/32', device=CONFIG['device'])\n",
    "        self.mask_generator = SamAutomaticMaskGenerator(self.sam)\n",
    "        \n",
    "    def _clip_similarity(self, image_patch):\n",
    "        # Convert numpy array to PIL Image\n",
    "        if isinstance(image_patch, np.ndarray):\n",
    "            if image_patch.shape[-1] == 3:  # BGR to RGB\n",
    "                image_patch = cv2.cvtColor(image_patch, cv2.COLOR_BGR2RGB)\n",
    "            image_patch = Image.fromarray(image_patch.astype('uint8'))\n",
    "            \n",
    "        image_tensor = self.preprocess(image_patch).unsqueeze(0).to(CONFIG['device'])\n",
    "        text = clip.tokenize([\"Urban street scene with vehicles, pedestrians, and road infrastructure\"]).to(CONFIG['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image_tensor)\n",
    "            text_features = self.clip_model.encode_text(text)\n",
    "            \n",
    "        return torch.cosine_similarity(image_features, text_features).item()\n",
    "    \n",
    "    def generate(self, image_paths):\n",
    "        pseudo_labels = []\n",
    "        \n",
    "        for path in tqdm(image_paths, desc=\"Generating pseudo-labels\"):\n",
    "            image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            masks = self.mask_generator.generate(image)\n",
    "            \n",
    "            valid_masks = []\n",
    "            for mask in masks:\n",
    "                x1, y1, w, h = mask['bbox']\n",
    "                if w == 0 or h == 0:\n",
    "                    continue  # Skip invalid masks\n",
    "                    \n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                patch = image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Handle empty patches\n",
    "                if patch.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                similarity = self._clip_similarity(patch)\n",
    "                if similarity > CONFIG['clip_threshold']:\n",
    "                    valid_masks.append(mask['segmentation'])\n",
    "            \n",
    "            if valid_masks:\n",
    "                combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                for m in valid_masks:\n",
    "                    combined_mask[m] = 1\n",
    "                pseudo_labels.append((path, combined_mask))\n",
    "        \n",
    "        return pseudo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|█████████████████████████████████████████████████████████| 20/20 [02:11<00:00,  6.56s/it]\n"
     ]
    }
   ],
   "source": [
    "pl_generator = PseudolabelGenerator()\n",
    "c_prime = pl_generator.generate(data.item_c[:20])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:  30%|█████████████████▍                                        | 6/20 [00:39<01:31,  6.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpl_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem_c\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mPseudolabelGenerator.generate\u001b[39m\u001b[34m(self, image_paths)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm(image_paths, desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating pseudo-labels\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     31\u001b[39m     image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     masks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     valid_masks = []\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator.generate\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m mask_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_mask_region_area > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._generate_masks\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    204\u001b[39m data = MaskData()\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     crop_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     data.cat(crop_data)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:245\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_crop\u001b[39m\u001b[34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[39m\n\u001b[32m    243\u001b[39m data = MaskData()\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m.points_per_batch, points_for_image):\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     batch_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     data.cat(batch_data)\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/segment_anything/automatic_mask_generator.py:297\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_batch\u001b[39m\u001b[34m(self, points, im_size, crop_box, orig_size)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pred_iou_thresh > \u001b[32m0.0\u001b[39m:\n\u001b[32m    296\u001b[39m     keep_mask = data[\u001b[33m\"\u001b[39m\u001b[33miou_preds\u001b[39m\u001b[33m\"\u001b[39m] > \u001b[38;5;28mself\u001b[39m.pred_iou_thresh\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Calculate stability score\u001b[39;00m\n\u001b[32m    300\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mstability_score\u001b[39m\u001b[33m\"\u001b[39m] = calculate_stability_score(\n\u001b[32m    301\u001b[39m     data[\u001b[33m\"\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m.predictor.model.mask_threshold, \u001b[38;5;28mself\u001b[39m.stability_score_offset\n\u001b[32m    302\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cva5/lib/python3.12/site-packages/segment_anything/utils/amg.py:49\u001b[39m, in \u001b[36mMaskData.filter\u001b[39m\u001b[34m(self, keep)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m._stats[k] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28mself\u001b[39m._stats[k] = v[torch.as_tensor(keep, device=v.device)]\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray):\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m._stats[k] = v[keep.detach().cpu().numpy()]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pl_generator.generate(data.item_c[:20])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, mask = self.data[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image = torch.tensor(image).permute(2,0,1).float() / 255.0\n",
    "        \n",
    "        mask = cv2.resize(mask, CONFIG['image_size'])\n",
    "        return image, torch.tensor(mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransUNetConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Existing parameters\n",
    "        self.img_size = kwargs.get('img_size', 512)\n",
    "        self.num_classes = kwargs.get('num_classes', 35)\n",
    "        self.hidden_size = kwargs.get('hidden_size', 768)\n",
    "        \n",
    "        # ResNet backbone parameters\n",
    "        self.resnet = {\n",
    "            'block_units': kwargs.get('block_units', [3, 4, 6, 3]),  # For ResNet-50\n",
    "            'width_factor': kwargs.get('width_factor', 4),\n",
    "            'pretrained_path': kwargs.get('pretrained_path', 'pretrained/resnet50.pth')\n",
    "        }\n",
    "        \n",
    "        # Transformer parameters\n",
    "        self.patches = {'grid': (16, 16)}\n",
    "        self.n_skip = 3\n",
    "        self.classifier = 'seg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    from networks.vit_seg_modeling_resnet_skip import ResNetV2\n",
    "    \n",
    "    config = TransUNetConfig(**CONFIG['transunet_config'])\n",
    "    \n",
    "    # Initialize ResNet with corrected parameters\n",
    "    resnet = ResNetV2(\n",
    "        block_units=config.resnet['block_units'],\n",
    "        width_factor=config.resnet['width_factor']\n",
    "    )\n",
    "    \n",
    "    # Load pretrained weights if available\n",
    "    if os.path.exists(config.resnet['pretrained_path']):\n",
    "        resnet.load_state_dict(torch.load(config.resnet['pretrained_path']))\n",
    "    \n",
    "    # Rest of model initialization...\n",
    "\n",
    "\n",
    "    # Initialize VisionTransformer with ResNet\n",
    "    model = VisionTransformer(\n",
    "        config=config,\n",
    "        img_size=config.img_size,\n",
    "        num_classes=config.num_classes,\n",
    "        resnet=resnet\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    # Training setup\n",
    "    dataset = SegmentationDataset(train_data)\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Initialize epoch loss\n",
    "        \n",
    "        for images, masks in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(CONFIG['device'])\n",
    "            masks = masks.to(CONFIG['device'])\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(loader):.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VisionTransformer.__init__() got an unexpected keyword argument 'resnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_prime\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_data)\u001b[39m\n\u001b[32m     14\u001b[39m     resnet.load_state_dict(torch.load(config.resnet[\u001b[33m'\u001b[39m\u001b[33mpretrained_path\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Rest of model initialization...\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Initialize VisionTransformer with ResNet\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresnet\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(CONFIG[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Training setup\u001b[39;00m\n\u001b[32m     28\u001b[39m dataset = SegmentationDataset(train_data)\n",
      "\u001b[31mTypeError\u001b[39m: VisionTransformer.__init__() got an unexpected keyword argument 'resnet'"
     ]
    }
   ],
   "source": [
    "model = train_model(c_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_paths):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "    \n",
    "    for path in tqdm(test_paths, desc=\"Evaluating\"):\n",
    "        # Load and prepare image\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, CONFIG['image_size'])\n",
    "        image_tensor = torch.tensor(image).permute(2,0,1).float().unsqueeze(0) / 255.0\n",
    "        image_tensor = image_tensor.to(CONFIG['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)  # Remove .logits if unnecessary\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        gt_path = path.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png')\n",
    "        gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask == gt_mask, gt_mask > 0)\n",
    "        union = np.logical_or(pred_mask > 0, gt_mask > 0)\n",
    "        \n",
    "        if np.sum(union) > 0:  \n",
    "            ious.append(np.sum(intersection) / np.sum(union))\n",
    "    \n",
    "    return {\n",
    "        'mean_iou': np.mean(ious),\n",
    "        'std_iou': np.std(ious),\n",
    "        'max_iou': np.max(ious),\n",
    "        'min_iou': np.min(ious)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, data.item_d[:10])  # Use subset for demo\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:8}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image_path, pred_mask, gt_mask):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image, CONFIG['image_size'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "    \n",
    "    ax[0].imshow(image_resized)\n",
    "    ax[0].set_title('Input Image')\n",
    "    \n",
    "    ax[1].imshow(pred_mask)\n",
    "    ax[1].set_title('Predicted Mask')\n",
    "    \n",
    "    ax[2].imshow(gt_mask)\n",
    "    ax[2].set_title('Ground Truth')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = data.item_d[0]\n",
    "pred_mask = model(torch.tensor(cv2.imread(test_image)).permute(2,0,1).float().unsqueeze(0).to(CONFIG['device'])/255.0).logits.argmax(1).squeeze().cpu().numpy()\n",
    "gt_mask = cv2.imread(test_image.replace('leftImg8bit', 'gtFine').replace('.png', '_gtFine_labelIds.png'), cv2.IMREAD_GRAYSCALE)\n",
    "gt_mask = cv2.resize(gt_mask, CONFIG['image_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample(test_image, pred_mask, gt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
